<h1 id="ml-technique-t-sne">ML Technique: T-SNE</h1>
<p>T-sne is a dimension reduction algorithm. It’s best used to visualize data with lots of features in 2D space.</p>
<h1 id="dataset-nki-breast-cancer-data">Dataset: NKI Breast Cancer Data</h1>
<p>To demonstrate through example, we will use the NKI Breast Cancer Dataset <a href="https://www.google.com/url?q=https://data.world/deviramanan2016/nki-breast-cancer-data&amp;sa=D&amp;source=docs&amp;ust=1660151201266258&amp;usg=AOvVaw3zOXI33wxSbCse84FI3RYY">[1]</a>. This dataset contains gene transcription information for 273 samples. There are 3 patient attributes, 10 discrete clinical attributes, and 1554 gene attributes. Each gene attribute is a normalized ratio using common gene expression profiling techniques <a href="https://www.nature.com/articles/415530a">[2]</a>.</p>
<h1 id="low-dimentional-visualizations">Low Dimentional Visualizations</h1>
<p>There are many questions we could ask with such a rich dataset. “What genes correlate with each other? Can we predict survival based only on gene expression? What are the most important features when considering survival?” These questions have been addressed (to some extent) in research papers. <a href="https://www.scirp.org/journal/paperinformation.aspx?paperid=84902">Wu et. al.</a> applies all the common classical machine learning techniques such as PCA, linear regression, SVM, random forest, and logistic regression. <a href="https://d1bp1ynq8xms31.cloudfront.net/wp-content/uploads/2015/02/Topology_Based_Data_Analysis_Identifies_a_Subgroup_of_Breast_Cancer_with_a_unique_mutational_profile_and_excellent_survival.pdf">Nicolaua et. al.</a> uses a more advanced topology method to discover a subset of genes (c-MYB+) that, when exhibited, correlate to a 100% chance of survival. They are of a bit different caliber, but well worth a quick read if you would like to learn more about the data.</p>
<p>Today we will focus on the best way to visualize this data. Of course, there are many ways to visualize one feature at a time. Here are two ways to show the distribution of age and survival time, respectively:</p>
<p>{ inset box plot of age and hist of survival }</p>
<p>We can also fairly easily plot 2 dimensions of data at a time:</p>
<p>{ inset scatter plot and side box plots}</p>
<p>Taking a look at one gene: <code>Contig51749_NC</code>, we can see how the expression of this gene corresponds to the survival time or survival rate.</p>
<p>In reality, it would be much more useful to take in account all the genes. To do that, we will use T-SNE.</p>
<h1 id="t-sne-explained">T-SNE explained</h1>
<p><em>Here, I attempt to summarize a <a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">state-of-the-art 27-page paper</a> in a few short paragraphs. There are many <a href="https://www.linkedin.com/pulse/visualization-method-sne-vs-t-sne-implementation-using-tandia">blogs</a>, <a href="https://distill.pub/2016/misread-tsne/?_ga=2.135835192.888864733.1531353600-1779571267.1531353600">websites</a>, and <a href="https://youtu.be/NEaUSP4YerM">videos</a> that do this better than I. I recommend checking them out too!</em></p>
<h2 id="determining-similarity-of-points-in-original-space">Determining Similarity of Points in Original Space</h2>
<p>We would like similar points to be plotted next to each other, and dissimilar points to be far away. A point is similar if its features are close to each other, relative to all other points. This is exactly what Stochastic Neighbor Embedding calculates.</p>
<p>First, we calculate the conditional probability of j given i by finding the euclidian distance between the two points xi and xj, divided by a smoothing constant. In order to know whether this distance is actually “close” or “far”, we find the distance of all the other points k and sum them.</p>
<p><span class="math display">\[ p*{j|i|}=\frac{exp(\frac{-\left\| x_i-x_j\right\|^2}{2\sigma_i^2})}{\sum*{k\neq i} exp(\frac{-\left\| x_i-x_k \right\|^2}{2\sigma_i^2})} \]</span></p>
<p>We also can imagine a case where the probability of <span class="math inline">\(p_{j|i}\)</span> is different from <span class="math inline">\(p_{i|j}\)</span>. To account, we just take the average of the two probabilities.</p>
<p><span class="math display">\[ p*{ij} = \frac{p*{j|i} + p\_{i|j}}{2n} \]</span></p>
<h2 id="positioning-the-points-in-the-new-space">Positioning the Points in the New Space</h2>
<p>Second, we apply the same ratio in the lower-dimentional space. Notice, the <span class="math inline">\(\sigma^2\)</span> constant is taken out. Remember that this was simply a smoothing constant (perplexity) that doesn’t make sense to include in the new space.</p>
<p><span class="math display">\[ q*{ij}=\frac{exp(-\left\| y_i -y_j \right\|^2)}{\sum*{k \neq l} exp(-\left\| y_k -y_l \right\|^2)} \]</span></p>
<p>Above is the equation for SNE, which uses a Cauchy distribution. To make this “t-SNE”, we will use a Student t-distribution with one degree of freedom.</p>
<p><span class="math display">\[ q*{ij}=\frac{(1+\left\| y_i -y_j \right\|^2)^{-1}}{\sum*{k \neq l}(1+\left\| y_k -y_l \right\|^2)^{-1}} \]</span></p>
<h2 id="iterating-over-the-cost-function">Iterating over the Cost Function</h2>
<p>Now we have the probability that the two points are close to each other in original space ( <span class="math inline">\(p_{ij}\)</span> ) and we have a prediction of how close they should be in low-D space (<span class="math inline">\(q_{ij}\)</span>). If this was a perfect match $ p<em>{ij} $ and $ q</em>{ij} $ would equal each other.</p>
<p>So we can put this in our cost function. The author of the original paper chose to use the of Kullback-Leibler divergence. Notice how the log in this function. This means that, given the choice between making sure similar points stay together, or similar points are far apart, the function favors clustering similar points together.</p>
<p>Our cost function is:</p>
<p><span class="math display">\[C = KL(P||Q)=\sum_i\sum_jp_{ij}log\frac{p_{ij}}{q_{ij}} \]</span></p>
<p>What’s actually really nice about this cost function is that it’s gradient decent w.r.t. <span class="math inline">\(y\)</span> is simply:</p>
<p><span class="math display">\[ \frac{\partial C}{\partial y*i} = 4\sum_j(p*{ij}-q\_{ij})(y_i-y_j) \]</span></p>
<h2 id="quick-summary">Quick Summary</h2>
<p>To review, we have a dataset that has many features. We calculate <span class="math inline">\(p_{ij}\)</span> which is a ratio that is large if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are close to each other and small if they are far apart. Then we sample an initial solution for the low-dimensional points $ q*{ij} $. We make incremental improvements to <span class="math inline">\(q*{ij}\)</span> by minimizing the cost function. The cost function will try to move similar points closer together, while pushing dissimilar points farther away. This push and pulling will eventually (hopefully) create the same clusters seen in high-D space in low-D space.</p>
<h1 id="using-sci-kit-learns-t-sne">Using sci-kit Learn’s T-SNE</h1>
<p>One day I would like to implement T-SNE from scratch in python as <a href="https://www.linkedin.com/pulse/visualization-method-sne-vs-t-sne-implementation-using-tandia/">M. Farhan Tandia did here</a>. Until then, we will use sci-kit learn’s T-SNE api. As a bonus, also allows us to easily compare other visualization methods.</p>
